# Multi-Model AI Setup ðŸ¤–

This guide explains the new multi-model feature that allows users to select from OpenRouter's extensive catalog of AI models.

## Features

### 1. **Dynamic Model Loading**

- Automatically fetches available models from OpenRouter API
- Models are cached for 1 hour to minimize API calls
- Fallback to popular models if API is unavailable

### 2. **Model Selection UI**

- Beautiful dropdown selector in the Inspector panel
- Shows model name, description, context length, and pricing info
- Real-time model information display

### 3. **Supported Models**

Out of the box, includes:

- **Anthropic**: Claude 3.5 Sonnet, Claude 3 Opus, Claude 3 Haiku
- **OpenAI**: GPT-4 Turbo, GPT-4, GPT-3.5 Turbo
- **Meta**: Llama 3.1 (70B, 8B)
- **Mistral**: Mistral 7B Instruct, Mistral Large
- **And 100+ more models from OpenRouter**

## Architecture

### Backend Changes

#### New Endpoint: `GET /api/models`

```javascript
GET http://localhost:3001/api/models
```

**Response:**

```json
{
  "models": [
    {
      "id": "anthropic/claude-3.5-sonnet",
      "name": "Claude 3.5 Sonnet",
      "description": "Most capable Claude model",
      "context_length": 200000,
      "pricing": {
        "prompt": "0.003",
        "completion": "0.015"
      }
    },
    ...
  ],
  "count": 150,
  "fallback": false
}
```

### Frontend Changes

#### 1. **New Files**

- `src/utils/openrouterModels.ts` - Model fetching and caching utilities
- `src/components/ModelSelector.tsx` - Dropdown component for model selection

#### 2. **Updated Store**

`src/store/flowStore.ts` now includes:

```typescript
models: ModelInfo[];
loadingModels: boolean;
fetchModels(): Promise<void>;
setModels(models: ModelInfo[]): void;
setLoadingModels(loading: boolean): void;
```

#### 3. **Enhanced Inspector**

`src/components/NodeInspectorPanel.tsx` now uses `ModelSelector` component for the Model block, replacing the plain text input.

#### 4. **Auto-initialization**

`src/App.tsx` calls `fetchModels()` on app load to populate the model list.

## Usage

### For Users

1. **Select a Model Block** in the canvas
2. **Open the Inspector Panel** (right sidebar)
3. **Click the AI Model dropdown** to see all available models
4. **Choose your preferred model** - see pricing and context length info
5. **Configure temperature and max tokens** as needed
6. **Run the flow** - the selected model will be used

### For Developers

#### Adding Custom Models

Edit `src/utils/openrouterModels.ts`:

```typescript
function getPopularModels(): ModelInfo[] {
  return [
    {
      id: "your-provider/your-model",
      name: "Your Model Name",
      description: "Your description",
    },
    // ... other models
  ];
}
```

#### Accessing Models in Code

```typescript
const { models, fetchModels, loadingModels } = useFlowStore();

// Fetch models
await fetchModels();

// Use models
models.forEach((model) => {
  console.log(`${model.name} (${model.id})`);
});
```

## API Integration

### OpenRouter Model Fetching

The backend fetches from: `https://openrouter.ai/api/v1/models`

**Note:** This endpoint is public and requires no authentication. However, to execute AI requests, you need a valid `OPENROUTER_API_KEY`.

### Caching Strategy

Frontend caching (`src/utils/openrouterModels.ts`):

- **Cache Duration:** 1 hour
- **Storage:** Browser localStorage
- **Cache Key:** `openrouter_models_cache`

Backend fetching (`backend/server.js`):

- No caching (fresh models on each request)
- Fallback to popular models if OpenRouter API is unavailable

## Configuration

### Environment Variables

```env
OPENROUTER_API_KEY=your_api_key_here
YOUR_SITE_URL=http://localhost:3000
YOUR_SITE_NAME=BuildWithAi
```

### Model Selection Tips

**For Speed:**

- GPT-3.5 Turbo
- Mistral 7B

**For Quality:**

- Claude 3.5 Sonnet
- GPT-4 Turbo

**For Cost-Effectiveness:**

- Claude 3 Haiku
- Mistral Small

**For Open Source:**

- Llama 3.1 70B
- Mistral Large

## Testing

Run the test script to verify the models endpoint:

```bash
node test-models.js
```

This will:

1. Connect to the backend
2. Fetch the first 10 models
3. Display model details
4. Confirm the endpoint is working

## Troubleshooting

### No Models Loading

**Solution:** Ensure backend is running on port 3001

```bash
npm run dev
```

### Backend Error: "OpenRouter API error"

**Solution:**

1. Check internet connection
2. Verify OpenRouter API is accessible
3. Backend will use fallback models automatically

### Models Showing Old Data

**Solution:** Clear browser localStorage

```javascript
localStorage.removeItem("openrouter_models_cache");
```

### Model Not Working in Execution

**Solution:**

1. Verify `OPENROUTER_API_KEY` is set in `.env`
2. Check the selected model is valid in OpenRouter
3. Try using a different model

## Future Enhancements

### Planned Features

- [ ] Model search/filter by provider
- [ ] Model comparison tool
- [ ] Custom model pricing calculator
- [ ] Model performance benchmarks
- [ ] Favorite models bookmarking
- [ ] Model availability checker
- [ ] Alternative provider support (Azure, AWS, Replicate)

### Extensibility

The architecture is designed to support:

- Multiple AI providers
- Custom model registries
- Provider switching per block
- Regional model availability
- Model versioning

## Performance Metrics

- **Model Fetch Time:** ~500ms on first load
- **Cache Hit Time:** <10ms
- **Dropdown Render:** <50ms
- **Model Count:** 150+ models available

## Security Notes

- OpenRouter model listing endpoint is public (no key required)
- API key is only used for actual AI execution
- Model data is cached locally to reduce API calls
- No model data is sent to backend, only IDs

---

**For more information:** Visit [OpenRouter Documentation](https://openrouter.ai/docs)
